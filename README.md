# ğŸš€ Dockerfile Auto-Generation with Local LLMs (Ollama + EC2 Docker)

This project uses a locally hosted LLM (via Ollama) to **automatically generate a Dockerfile** for any DevOps project.  
Docker builds are executed on a remote AWS EC2 instance (where Docker is installed) via SSH tunneling from the local machine.

---

## ğŸ§± Project Structure


dockerfile-autogen-llm/
â”œâ”€â”€ main.py # Core script to generate Dockerfile using LLM
â”œâ”€â”€ prompt_template.txt # Prompt format template for the LLM
â”œâ”€â”€ Dockerfile # (Generated by LLM)
â””â”€â”€ sample-app/ # Sample DevOps project (e.g., Ansible scripts)


---

## âœ… Prerequisites

### ğŸ”§ Local (Windows)

- Python 3.11+
- [Ollama](https://ollama.com) installed on Windows
- WSL
- `llama3.2` model pulled via:
  ```bash
  ollama pull llama3.2


# ğŸš€ Dockerfile Auto-Generation with Local LLMs (Ollama + EC2 Docker)

This project uses a locally hosted LLM (via Ollama) to **automatically generate a Dockerfile** for any DevOps project.  
Docker builds are executed on a remote AWS EC2 instance (where Docker is installed) via SSH tunneling from the local machine.

---

## ğŸ§± Project Structure

dockerfile-autogen-llm/
â”œâ”€â”€ main.py # Core script to generate Dockerfile using LLM
â”œâ”€â”€ prompt_template.txt # Prompt format template for the LLM
â”œâ”€â”€ Dockerfile # (Generated by LLM)
â””â”€â”€ sample-app/ # Sample DevOps project (e.g., Ansible scripts)

---

## âœ… Prerequisites

### ğŸ”§ Local (Windows)

- Python 3.11+
- [Ollama](https://ollama.com) installed on Windows
- `llama3.2` model pulled via:  
  ```bash
  ollama pull llama3.2
WSL2 enabled

SSH key (LLM.pem) to access EC2

ğŸ§ WSL (Ubuntu Terminal)
Install dependencies:

  ```bash
  sudo apt update && sudo apt install tree autossh -y
  sudo apt update
  sudo apt install -y docker-cli
  ```

SSH access to EC2 (with Docker installed)

â˜ï¸ AWS EC2 (Ubuntu Instance)
Docker installed:

  ```bash
  sudo apt update && sudo apt install docker.io -y
  sudo usermod -aG docker ubuntu
  newgrp docker
  Docker daemon listening over TCP:
  ```

Edit:

  ```bash
  sudo vi nano /lib/systemd/system/docker.service
  ```

find line:
  ExecStart=/usr/bin/dockerd -H fd://

update above with new line:
  ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375

Reload:

  ```bash
  sudo systemctl daemon-reexec
  sudo systemctl daemon-reload
  sudo systemctl restart docker
  ```

ğŸš€ How to Use
1. Start SSH Tunnel (from WSL)

  ```bash
  ssh -i ~/.ssh/LLM.pem -NL localhost:2375:/var/run/docker.sock ubuntu@<EC2_PUBLIC_IP>
  ```
ğŸ“Œ Leave this terminal open during the entire operation.

2. In Another WSL Terminal

  ```bash
  export DOCKER_HOST=tcp://localhost:2375
  docker info       #  Ensure Docker is reachable
  ```
3. Run LLM to Generate Dockerfile


  ```bash
  cd /mnt/c/.../dockerfile-autogen-llm
  python3 main.py
  ```
âœ”ï¸ This will:

Parse the sample-app structure

Format a prompt using prompt_template.txt

Run prompt via ollama

Save Dockerfile locally

4. Build Docker Image Remotely

  ```bash
  docker build -t devops-image -f Dockerfile .
  ```

ğŸ§  File Explanations
File	Purpose

main.py	Core Python script: generates prompt, calls LLM, saves Dockerfile
prompt_template.txt	Prompt format with placeholders ({file_structure}, {project_description})
Dockerfile	Generated by main.py using LLM output
sample-app/	Sample DevOps project folder (e.g., Ansible setup)


âš ï¸ Common Errors & Solutions
âŒ Error: ğŸ” SSH Tunnel Disconnects or Fails

âœ… Solution: Ensure the SSH tunnel is actively running:


  ```bash
  ssh -i ~/.ssh/LLM.pem -NL localhost:2375:/var/run/docker.sock ubuntu@<EC2_PUBLIC_IP>
  ```
ğŸ’¡ Tip: use autossh for persistent tunnel:
  ```bash
  sudo apt install autossh
  autossh -M 0 -f -N -i ~/.ssh/LLM.pem -L 2375:/var/run/docker.sock ubuntu@13.233.194.42
  ```

ğŸ³ Docker Build Hangs or Fails
âŒ Issue:
Empty Dockerfile

Invalid paths like COPY --from=builder ... when no multistage

âœ… Solution:
Check Dockerfile content:

  ```bash
  cat Dockerfile
  ```
  Validate prompt logic in main.py

Ensure sample-app/ has valid files: e.g. main.py, requirements.txt, etc.

ğŸ§  LLM Doesnâ€™t Respond Properly
âŒ Error:
âŒ LLM returned no output.

âœ… Solution:
Ensure model is pulled:
Check prompt_template.txt format
Try direct prompt via CLI:

  ```bash
  ollama run llama3.2
  ```
ğŸ“¦ Future Additions
âœ… Datadog agent integration
ğŸ”„ Live prompt tuning
ğŸ“¦ CI/CD integration

ğŸ§  Model performance evaluation

âœ¨ Example Usage
  ```bash
  cd dockerfile-autogen-llm/
  python3 main.py
  docker build -t devops-image -f Dockerfile .
  docker run devops-image
  ```

ğŸ‘¨â€ğŸ’» Author
Sujitkumar hujare
DevOps|Automation|GenAI|LLM
inkedin.com/in/sujitkumar-hujare/
