# ğŸš€ Dockerfile Auto-Generation with Local LLMs (Ollama + EC2 Docker)

This project uses a locally hosted LLM (via Ollama) to **automatically generate a Dockerfile** for any DevOps project.  
Docker builds are executed on a remote AWS EC2 instance (where Docker is installed) via SSH tunneling from the local machine.

---

## ğŸ§± Project Structure

dockerfile-autogen-llm/
â”œâ”€â”€ main.py # Core script to generate Dockerfile using LLM
â”œâ”€â”€ prompt_template.txt # Prompt format template for the LLM
â”œâ”€â”€ Dockerfile # (Generated by LLM)
â””â”€â”€ sample-app/ # Sample DevOps project (e.g., Ansible scripts)



---

## âœ… Prerequisites

### ğŸ”§ Local (Windows)

- Python 3.11+
- [Ollama](https://ollama.com) installed on Windows
- `llama3.2` model pulled via:
  ```bash
  ollama pull llama3.2


# ğŸš€ Dockerfile Auto-Generation with Local LLMs (Ollama + EC2 Docker)

This project uses a locally hosted LLM (via Ollama) to **automatically generate a Dockerfile** for any DevOps project.  
Docker builds are executed on a remote AWS EC2 instance (where Docker is installed) via SSH tunneling from the local machine.

---

## ğŸ§± Project Structure

dockerfile-autogen-llm/
â”œâ”€â”€ main.py # Core script to generate Dockerfile using LLM
â”œâ”€â”€ prompt_template.txt # Prompt format template for the LLM
â”œâ”€â”€ Dockerfile # (Generated by LLM)
â””â”€â”€ sample-app/ # Sample DevOps project (e.g., Ansible scripts)

yaml
Copy
Edit

---

## âœ… Prerequisites

### ğŸ”§ Local (Windows)

- Python 3.11+
- [Ollama](https://ollama.com) installed on Windows
- `llama3.2` model pulled via:  
  ```bash
  ollama pull llama3.2
WSL2 enabled

SSH key (LLM.pem) to access EC2

ğŸ§ WSL (Ubuntu Terminal)
Install dependencies:

bash
Copy
Edit
sudo apt update && sudo apt install tree autossh -y
SSH access to EC2 (with Docker installed)

â˜ï¸ AWS EC2 (Ubuntu Instance)
Docker installed:

bash
Copy
Edit
sudo apt update && sudo apt install docker.io -y
sudo usermod -aG docker ubuntu
newgrp docker
Docker daemon listening over TCP:
Edit:

bash
Copy
Edit
sudo nano /lib/systemd/system/docker.service
Update line:

ini
Copy
Edit
ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375
Reload:

bash
Copy
Edit
sudo systemctl daemon-reexec
sudo systemctl daemon-reload
sudo systemctl restart docker
ğŸš€ How to Use
1. Start SSH Tunnel (from WSL)
bash
Copy
Edit
ssh -i ~/.ssh/LLM.pem -NL localhost:2375:/var/run/docker.sock ubuntu@<EC2_PUBLIC_IP>
ğŸ“Œ Leave this terminal open during the entire operation.

2. In Another WSL Terminal
bash
Copy
Edit
export DOCKER_HOST=tcp://localhost:2375
docker info       # âœ… Ensure Docker is reachable
3. Run LLM to Generate Dockerfile
bash
Copy
Edit
cd /mnt/c/.../dockerfile-autogen-llm
python3 main.py
âœ”ï¸ This will:

Parse the sample-app structure

Format a prompt using prompt_template.txt

Run prompt via ollama

Save Dockerfile locally

4. Build Docker Image Remotely
bash
Copy
Edit
docker build -t devops-image -f Dockerfile .
ğŸ§  File Explanations
File	Purpose
main.py	Core Python script: generates prompt, calls LLM, saves Dockerfile
prompt_template.txt	Prompt format with placeholders ({file_structure}, {project_description})
Dockerfile	Generated by main.py using LLM output
sample-app/	Sample DevOps project folder (e.g., Ansible setup)

âš ï¸ Common Errors & Solutions
ğŸ” SSH Tunnel Disconnects or Fails
âŒ Error:
pgsql
Copy
Edit
write: connection reset by peer
âœ… Solution:
Ensure the SSH tunnel is actively running:

bash
Copy
Edit
ssh -i ~/.ssh/LLM.pem -NL localhost:2375:/var/run/docker.sock ubuntu@<EC2_PUBLIC_IP>
ğŸ’¡ Tip: use autossh for persistent tunnel:

bash
Copy
Edit
autossh -M 0 -f -N -i ~/.ssh/LLM.pem -L 2375:/var/run/docker.sock ubuntu@<EC2_PUBLIC_IP>
ğŸ³ Docker Build Hangs or Fails
âŒ Issue:
Empty Dockerfile

Invalid paths like COPY --from=builder ... when no multistage

âœ… Solution:
Check Dockerfile content:

bash
Copy
Edit
cat Dockerfile
Validate prompt logic in main.py

Ensure sample-app/ has valid files: e.g. main.py, requirements.txt, etc.

ğŸ§  LLM Doesnâ€™t Respond Properly
âŒ Error:
bash
Copy
Edit
âŒ LLM returned no output.
âœ… Solution:
Ensure model is pulled:

bash
Copy
Edit
ollama pull llama3.2
Check prompt_template.txt format

Try direct prompt via CLI:

bash
Copy
Edit
ollama run llama3.2
ğŸ“¦ Future Additions
âœ… Datadog agent integration

ğŸ”„ Live prompt tuning

ğŸ“¦ CI/CD integration

ğŸ§  Model performance evaluation

âœ¨ Example Usage
bash
Copy
Edit
cd dockerfile-autogen-llm/
python3 main.py
docker build -t devops-image -f Dockerfile .
docker run devops-image
ğŸ‘¨â€ğŸ’» Author
Sujitkumar hujare
DevOps|Automation|GenAI|LLM

