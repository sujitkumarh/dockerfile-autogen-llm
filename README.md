# 🚀 Dockerfile Auto-Generation with Local LLMs (Ollama + EC2 Docker)

This project uses a locally hosted LLM (via Ollama) to **automatically generate a Dockerfile** for any DevOps project.  
Docker builds are executed on a remote AWS EC2 instance (where Docker is installed) via SSH tunneling from the local machine.

---

## 🧱 Project Structure

dockerfile-autogen-llm/
├── main.py # Core script to generate Dockerfile using LLM
├── prompt_template.txt # Prompt format template for the LLM
├── Dockerfile # (Generated by LLM)
└── sample-app/ # Sample DevOps project (e.g., Ansible scripts)



---

## ✅ Prerequisites

### 🔧 Local (Windows)

- Python 3.11+
- [Ollama](https://ollama.com) installed on Windows
- `llama3.2` model pulled via:
  ```bash
  ollama pull llama3.2


# 🚀 Dockerfile Auto-Generation with Local LLMs (Ollama + EC2 Docker)

This project uses a locally hosted LLM (via Ollama) to **automatically generate a Dockerfile** for any DevOps project.  
Docker builds are executed on a remote AWS EC2 instance (where Docker is installed) via SSH tunneling from the local machine.

---

## 🧱 Project Structure

dockerfile-autogen-llm/
├── main.py # Core script to generate Dockerfile using LLM
├── prompt_template.txt # Prompt format template for the LLM
├── Dockerfile # (Generated by LLM)
└── sample-app/ # Sample DevOps project (e.g., Ansible scripts)

yaml
Copy
Edit

---

## ✅ Prerequisites

### 🔧 Local (Windows)

- Python 3.11+
- [Ollama](https://ollama.com) installed on Windows
- `llama3.2` model pulled via:  
  ```bash
  ollama pull llama3.2
WSL2 enabled

SSH key (LLM.pem) to access EC2

🐧 WSL (Ubuntu Terminal)
Install dependencies:

bash
Copy
Edit
sudo apt update && sudo apt install tree autossh -y
SSH access to EC2 (with Docker installed)

☁️ AWS EC2 (Ubuntu Instance)
Docker installed:

bash
Copy
Edit
sudo apt update && sudo apt install docker.io -y
sudo usermod -aG docker ubuntu
newgrp docker
Docker daemon listening over TCP:
Edit:

bash
Copy
Edit
sudo nano /lib/systemd/system/docker.service
Update line:

ini
Copy
Edit
ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375
Reload:

bash
Copy
Edit
sudo systemctl daemon-reexec
sudo systemctl daemon-reload
sudo systemctl restart docker
🚀 How to Use
1. Start SSH Tunnel (from WSL)
bash
Copy
Edit
ssh -i ~/.ssh/LLM.pem -NL localhost:2375:/var/run/docker.sock ubuntu@<EC2_PUBLIC_IP>
📌 Leave this terminal open during the entire operation.

2. In Another WSL Terminal
bash
Copy
Edit
export DOCKER_HOST=tcp://localhost:2375
docker info       # ✅ Ensure Docker is reachable
3. Run LLM to Generate Dockerfile
bash
Copy
Edit
cd /mnt/c/.../dockerfile-autogen-llm
python3 main.py
✔️ This will:

Parse the sample-app structure

Format a prompt using prompt_template.txt

Run prompt via ollama

Save Dockerfile locally

4. Build Docker Image Remotely
bash
Copy
Edit
docker build -t devops-image -f Dockerfile .
🧠 File Explanations
File	Purpose
main.py	Core Python script: generates prompt, calls LLM, saves Dockerfile
prompt_template.txt	Prompt format with placeholders ({file_structure}, {project_description})
Dockerfile	Generated by main.py using LLM output
sample-app/	Sample DevOps project folder (e.g., Ansible setup)

⚠️ Common Errors & Solutions
🔁 SSH Tunnel Disconnects or Fails
❌ Error:
pgsql
Copy
Edit
write: connection reset by peer
✅ Solution:
Ensure the SSH tunnel is actively running:

bash
Copy
Edit
ssh -i ~/.ssh/LLM.pem -NL localhost:2375:/var/run/docker.sock ubuntu@<EC2_PUBLIC_IP>
💡 Tip: use autossh for persistent tunnel:

bash
Copy
Edit
autossh -M 0 -f -N -i ~/.ssh/LLM.pem -L 2375:/var/run/docker.sock ubuntu@<EC2_PUBLIC_IP>
🐳 Docker Build Hangs or Fails
❌ Issue:
Empty Dockerfile

Invalid paths like COPY --from=builder ... when no multistage

✅ Solution:
Check Dockerfile content:

bash
Copy
Edit
cat Dockerfile
Validate prompt logic in main.py

Ensure sample-app/ has valid files: e.g. main.py, requirements.txt, etc.

🧠 LLM Doesn’t Respond Properly
❌ Error:
bash
Copy
Edit
❌ LLM returned no output.
✅ Solution:
Ensure model is pulled:

bash
Copy
Edit
ollama pull llama3.2
Check prompt_template.txt format

Try direct prompt via CLI:

bash
Copy
Edit
ollama run llama3.2
📦 Future Additions
✅ Datadog agent integration

🔄 Live prompt tuning

📦 CI/CD integration

🧠 Model performance evaluation

✨ Example Usage
bash
Copy
Edit
cd dockerfile-autogen-llm/
python3 main.py
docker build -t devops-image -f Dockerfile .
docker run devops-image
👨‍💻 Author
Sujitkumar hujare
DevOps|Automation|GenAI|LLM

