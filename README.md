# 🚀 Dockerfile Auto-Generation with Local LLMs (Ollama + EC2 Docker)

This project uses a locally hosted LLM (via Ollama) to **automatically generate a Dockerfile** for any DevOps project.  
Docker builds are executed on a remote AWS EC2 instance (where Docker is installed) via SSH tunneling from the local machine.

---

## 🧱 Project Structure


dockerfile-autogen-llm/
├── main.py # Core script to generate Dockerfile using LLM
├── prompt_template.txt # Prompt format template for the LLM
├── Dockerfile # (Generated by LLM)
└── sample-app/ # Sample DevOps project (e.g., Ansible scripts)


---

## ✅ Prerequisites

### 🔧 Local (Windows)

- Python 3.11+
- [Ollama](https://ollama.com) installed on Windows
- WSL
- `llama3.2` model pulled via:
  ```bash
  ollama pull llama3.2


# 🚀 Dockerfile Auto-Generation with Local LLMs (Ollama + EC2 Docker)

This project uses a locally hosted LLM (via Ollama) to **automatically generate a Dockerfile** for any DevOps project.  
Docker builds are executed on a remote AWS EC2 instance (where Docker is installed) via SSH tunneling from the local machine.

---

## 🧱 Project Structure

dockerfile-autogen-llm/
├── main.py # Core script to generate Dockerfile using LLM
├── prompt_template.txt # Prompt format template for the LLM
├── Dockerfile # (Generated by LLM)
└── sample-app/ # Sample DevOps project (e.g., Ansible scripts)

---

## ✅ Prerequisites

### 🔧 Local (Windows)

- Python 3.11+
- [Ollama](https://ollama.com) installed on Windows
- `llama3.2` model pulled via:  
  ```bash
  ollama pull llama3.2
WSL2 enabled

SSH key (LLM.pem) to access EC2

🐧 WSL (Ubuntu Terminal)
Install dependencies:

  ```bash
  sudo apt update && sudo apt install tree autossh -y
  sudo apt update
  sudo apt install -y docker-cli
  ```

SSH access to EC2 (with Docker installed)

☁️ AWS EC2 (Ubuntu Instance)
Docker installed:

  ```bash
  sudo apt update && sudo apt install docker.io -y
  sudo usermod -aG docker ubuntu
  newgrp docker
  Docker daemon listening over TCP:
  ```

Edit:

  ```bash
  sudo vi nano /lib/systemd/system/docker.service
  ```

find line:
  ExecStart=/usr/bin/dockerd -H fd://

update above with new line:
  ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375

Reload:

  ```bash
  sudo systemctl daemon-reexec
  sudo systemctl daemon-reload
  sudo systemctl restart docker
  ```

🚀 How to Use
1. Start SSH Tunnel (from WSL)

  ```bash
  ssh -i ~/.ssh/LLM.pem -NL localhost:2375:/var/run/docker.sock ubuntu@<EC2_PUBLIC_IP>
  ```
📌 Leave this terminal open during the entire operation.

2. In Another WSL Terminal

  ```bash
  export DOCKER_HOST=tcp://localhost:2375
  docker info       #  Ensure Docker is reachable
  ```
3. Run LLM to Generate Dockerfile


  ```bash
  cd /mnt/c/.../dockerfile-autogen-llm
  python3 main.py
  ```
✔️ This will:

Parse the sample-app structure

Format a prompt using prompt_template.txt

Run prompt via ollama

Save Dockerfile locally

4. Build Docker Image Remotely

  ```bash
  docker build -t devops-image -f Dockerfile .
  ```

🧠 File Explanations
File	Purpose

main.py	Core Python script: generates prompt, calls LLM, saves Dockerfile
prompt_template.txt	Prompt format with placeholders ({file_structure}, {project_description})
Dockerfile	Generated by main.py using LLM output
sample-app/	Sample DevOps project folder (e.g., Ansible setup)


⚠️ Common Errors & Solutions
❌ Error: 🔁 SSH Tunnel Disconnects or Fails

✅ Solution: Ensure the SSH tunnel is actively running:


  ```bash
  ssh -i ~/.ssh/LLM.pem -NL localhost:2375:/var/run/docker.sock ubuntu@<EC2_PUBLIC_IP>
  ```
💡 Tip: use autossh for persistent tunnel:
  ```bash
  sudo apt install autossh
  autossh -M 0 -f -N -i ~/.ssh/LLM.pem -L 2375:/var/run/docker.sock ubuntu@13.233.194.42
  ```

🐳 Docker Build Hangs or Fails
❌ Issue:
Empty Dockerfile

Invalid paths like COPY --from=builder ... when no multistage

✅ Solution:
Check Dockerfile content:

  ```bash
  cat Dockerfile
  ```
  Validate prompt logic in main.py

Ensure sample-app/ has valid files: e.g. main.py, requirements.txt, etc.

🧠 LLM Doesn’t Respond Properly
❌ Error:
❌ LLM returned no output.

✅ Solution:
Ensure model is pulled:
Check prompt_template.txt format
Try direct prompt via CLI:

  ```bash
  ollama run llama3.2
  ```
📦 Future Additions
✅ Datadog agent integration
🔄 Live prompt tuning
📦 CI/CD integration

🧠 Model performance evaluation

✨ Example Usage
  ```bash
  cd dockerfile-autogen-llm/
  python3 main.py
  docker build -t devops-image -f Dockerfile .
  docker run devops-image
  ```

👨‍💻 Author
Sujitkumar hujare
DevOps|Automation|GenAI|LLM
inkedin.com/in/sujitkumar-hujare/
